{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hunzlausman/ci2.2.6/blob/master/Llama_2_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et_BZnMBWzB-"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[torch]\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q langchain\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q llama-cpp-python==0.1.78\n",
        "# !pip install -q faiss-cpu\n",
        "!pip install -q faiss-gpu\n",
        "!pip install -q pypdf\n",
        "# !pip install -q pdfrw\n",
        "!pip install -q sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "#very important for utilizing gpu resources use this only if installed cpu version of llama-cpp-python already"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhX-AqiCftIV",
        "outputId": "32efb467-4c27-437d-efbd-c07e17b40887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting setuptools>=42\n",
            "    Using cached setuptools-68.1.2-py3-none-any.whl (805 kB)\n",
            "  Collecting scikit-build>=0.13\n",
            "    Using cached scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
            "  Collecting cmake>=3.18\n",
            "    Using cached cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
            "  Collecting ninja\n",
            "    Using cached ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "  Collecting distro (from scikit-build>=0.13)\n",
            "    Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
            "  Collecting packaging (from scikit-build>=0.13)\n",
            "    Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
            "  Collecting tomli (from scikit-build>=0.13)\n",
            "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
            "    Using cached wheel-0.41.2-py3-none-any.whl (64 kB)\n",
            "  Installing collected packages: ninja, cmake, wheel, tomli, setuptools, packaging, distro, scikit-build\n",
            "    Creating /tmp/pip-build-env-5jrhmun8/overlay/local/bin\n",
            "    changing mode of /tmp/pip-build-env-5jrhmun8/overlay/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-5jrhmun8/overlay/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-5jrhmun8/overlay/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-5jrhmun8/overlay/local/bin/ctest to 755\n",
            "    changing mode of /tmp/pip-build-env-5jrhmun8/overlay/local/bin/wheel to 755\n",
            "    changing mode of /tmp/pip-build-env-5jrhmun8/overlay/local/bin/distro to 755\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "  numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.2 which is incompatible.\n",
            "  tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\n",
            "  Successfully installed cmake-3.27.2 distro-1.8.0 ninja-1.11.1 packaging-23.1 scikit-build-0.17.6 setuptools-68.1.2 tomli-2.0.1 wheel-0.41.2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python.egg-info\n",
            "  writing /tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-7zc9fjvv/llama_cpp_python-0.1.78.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.1.78)\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "\n",
            "\n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Ninja' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "\n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Configuring done (0.5s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_cmake_test_compile/build\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Ninja' generator - success\n",
            "  --------------------------------------------------------------------------------\n",
            "\n",
            "  Configuring Project\n",
            "    Working directory:\n",
            "      /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-build\n",
            "    Command:\n",
            "      /tmp/pip-build-env-5jrhmun8/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-5jrhmun8/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-5jrhmun8/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-5jrhmun8/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
            "    Git repository not found; to enable automatic generation of build info,\n",
            "    make sure Git is installed and the project is a Git repository.\n",
            "\n",
            "\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/include (found version \"11.8.89\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 11.8.89\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Configuring done (4.4s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-build\n",
            "  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
            "  [3/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  [4/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  [6/9] Linking CXX shared library vendor/llama.cpp/libllama.so\n",
            "  [7/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a\n",
            "  [8/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so\n",
            "  [8/9] Install the project...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n",
            "  -- Set runtime path of \"/tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n",
            "  -- Installing: /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n",
            "  -- Set runtime path of \"/tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\" to \"\"\n",
            "\n",
            "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n",
            "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n",
            "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n",
            "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n",
            "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n",
            "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n",
            "  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n",
            "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n",
            "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n",
            "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n",
            "  copying /tmp/pip-install-pbg_jw1g/llama-cpp-python_c90793a0e2af437e83ac7b8fe2ca7c5f/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copied 9 files\n",
            "  running build_ext\n",
            "  installing to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copied 11 files\n",
            "  running install_data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  copied 0 files\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-wsknz4c2/.tmp-_mwot77p/llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'llama_cpp/__init__.py'\n",
            "  adding 'llama_cpp/libllama.so'\n",
            "  adding 'llama_cpp/llama.py'\n",
            "  adding 'llama_cpp/llama_cpp.py'\n",
            "  adding 'llama_cpp/llama_grammar.py'\n",
            "  adding 'llama_cpp/llama_types.py'\n",
            "  adding 'llama_cpp/py.typed'\n",
            "  adding 'llama_cpp/utils.py'\n",
            "  adding 'llama_cpp/server/__init__.py'\n",
            "  adding 'llama_cpp/server/__main__.py'\n",
            "  adding 'llama_cpp/server/app.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n",
            "  removing _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822210 sha256=f7c65d70a487f7dd42a43b3cc54510e4e82ed806d48c49e12990c8fe7e3680dd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yo6rzcib/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.7.1.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/bin/f2py3\n",
            "      Removing file or directory /usr/local/bin/f2py3.10\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.25.2.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  changing mode of /usr/local/bin/f2py3 to 755\n",
            "  changing mode of /usr/local/bin/f2py3.10 to 755\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama-cpp-python 0.1.78\n",
            "    Uninstalling llama-cpp-python-0.1.78:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp_python-0.1.78.dist-info/\n",
            "      Successfully uninstalled llama-cpp-python-0.1.78\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.25.2 typing-extensions-4.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA5Fg3BrhXiS",
        "outputId": "08d73e1a-a5d6-4d02-fc38-61bb38bc163d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python==0.1.78 in /usr/local/lib/python3.10/dist-packages (0.1.78)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input llama-2-7b-chat.ggmlv3.q4_K_M.bin --output llama-2-7b-chat.gguf.q4_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ErBvkLPiKbw",
        "outputId": "7b8b88c6-96b1-4b11-bc41-7cfb57012971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/./convert-llama-ggmlv3-to-gguf.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from huggingface_hub import hf_hub_download\n",
        "model_name_or_path = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "model_basename = \"llama-2-7b-chat.ggmlv3.q4_K_M.bin\"\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "FhocWpGvXZgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "# n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "# Loading model,\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "# llm.reset()\n",
        "# Loading model,\n",
        "# llm.reset()\n",
        "# llm.set_cache(None)\n",
        "llm = None\n",
        "del llm\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    max_tokens=1024,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        "    n_ctx=4096, # Context window\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    temperature = 0.4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaAmk04JXczS",
        "outputId": "bb7e0f6d-7e31-4a1d-fcee-edc724d63a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "7ebo1DoYXiRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=PyPDFLoader(\"Ch2.pdf\").load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "# retriever.reset()\n",
        "# retriever.set_cache(None)\n",
        "db = FAISS.from_documents(texts,HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",model_kwargs={'device':'cuda'}))\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "H3KEvHX1YTiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(retriever.get_relevant_documents(\"\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOj3QHL3Lk2d",
        "outputId": "78bd28c0-7c21-4f02-c413-0e7553ddd8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "template = \"\"\"''SYSTEM:You're a helpful assistant that can answer questions according to the following data:{content}\n",
        "USER: {query}\n",
        "ASSISTANT: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"query\",\"content\"])\n",
        "memory = ConversationSummaryMemory(llm=llm,return_messages=True)"
      ],
      "metadata": {
        "id": "WxCkFf2tYcMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Given the following history summary and a follow up question,if anything in the history summary relates to the original question rewrite follow up question to be a standalone question, in its original language.\n",
        "\n",
        "history summary:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate(template=template, input_variables=[\"chat_history\",\"question\"])\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "cbjzj4SFYl8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"what is my name?\"\n",
        "q_chain = LLMChain(prompt=CONDENSE_QUESTION_PROMPT, llm=llm)\n",
        "final_query = q_chain.run(chat_history=memory.chat_memory.messages,question=question)\n",
        "llm_chain = LLMChain(prompt=QA_PROMPT, llm=llm)\n",
        "docs = retriever.get_relevant_documents(final_query)\n",
        "print(docs)\n",
        "answer = llm_chain.run(context= docs,question=final_query)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSVlPgyFZibp",
        "outputId": "91eec4bf-fb1b-4be3-b0fa-86c5dccd5216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is your name?[Document(page_content='REPORT OF THE JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010   CHAPTER 2 CHAPTER 2   REPORT OF THE JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010   34 35\\nSr.No. Names of public complainants. \\n1.  Ch. Muhammad Yousuf, Advocate, Kot Adu (IW-24). \\n2.  Mr. Muhammad Younus Chandia s/o Ghulam Rasool, r/o Mauza Bhabar \\nGhair Mustaqil, Kot Adu (IW-25). \\n3.  Mr. Asghar Ali Khan Pachar, Advocate, r/o Village Chaudhry, Tehsil Kot Adu \\n(IW-26) \\n4.  \\n Syed Allah Bukhsh Shah r/o Tasneem Chah Basti Wazir Gadiwala, Mauza \\nTibba Mustaqil Sharqi, Kot Adu. (IW-27) \\n5.  Malik Munir Ahmad, Advocate, Kot Adu. (IW-28) \\n6.  Mr. Muhammad Ashraf Khan Rind s/o Manzoor Hussain Rind, former Nazim \\nUnion Council Bate Wala r/o Taunsa Barrage Colony, Tehsil Kot Adu. \\n(IW-29) \\n7.  \\n Mr. Ghulam Abbas s/o Muhammad Bukhsh, r/o Ward No.14-C, Mohallah \\nKhokhar Abad, Kot Adu (IW-31) \\n8.  \\n Mr. Rafique Ahmad Khan s/o Sardar Khan r/o Ward No.14-C, Kakkay Wala, \\nKot Adu. (IW-32) \\n9.  \\n Mr. Khalid Hussain Khan s/o Lal Muhammad Khan r/o Mauza Bate Qaim \\nWala, Tehsil Kot Adu. (IW-33) \\n10.  \\n Mr. Ghulam Abbas s/o Dost Muhammad r/o Chah Abbas Wala Mauza \\nHanjrai Ghair Mustaqil, Sharqi, Kot Adu.( IW-34) \\n11.  \\n Mr. Qadir Bukhsh s/o Hamid Khan, r/o Basti Nutkani, Mauza Hanjrai Ghair \\nMustaqil, Daira Deen Panah near LMB Abbas Wala, Kot Adu (IW-35) \\n12.  \\n Mr. Wahid Bukhsh s/o Ghulam Haider r/o Chah Abbas Wala, Mauza Hanjrai \\nGhair Mustaqil Sharqi, Kot Adu.  (IW-36) \\n13.  \\n Mr. Muhammad Mahiwal s/o Fateh Muhammad r/o Ward No.14-A, \\nMohallah Mandi Mawashian, Kot Adu (IW-37) \\n14.  Mr. Muhammad Bukhsh s/o Qadir Bukhsh r/o Chak Abbas Wala, Hanjrai \\nGhair Mustaqil Sharqi, Kot Addu. (IW-38) \\n \\n15.  Malik Muhammad Ibrahim Hanjra s/o Muhammad Ismail, Ex-Nazim, Union \\nCouncil Hanjra and News Reporter Nawa-e-Waqt, r/o Daira Deen Panah, \\nKot Adu. (IW-39) \\n \\n16.  \\n Mr. Sabir Hussain s/o Ghulam Sarwar r/o Mauza Tapal, Tehsil Kot Adu \\n(IW-40).  \\n17.  Mr. Ejaz Hussain s/o Ghulam Hassan r/o Chah Kandhi Wala, Mauza Kacha', metadata={'source': 'Ch2.pdf', 'page': 4}), Document(page_content='REPORT OF THE JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010   CHAPTER 2 CHAPTER 2   REPORT OF THE JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010   34 35\\nSr.No. Names of public complainants. \\n1.  Ch. Muhammad Yousuf, Advocate, Kot Adu (IW-24). \\n2.  Mr. Muhammad Younus Chandia s/o Ghulam Rasool, r/o Mauza Bhabar \\nGhair Mustaqil, Kot Adu (IW-25). \\n3.  Mr. Asghar Ali Khan Pachar, Advocate, r/o Village Chaudhry, Tehsil Kot Adu \\n(IW-26) \\n4.  \\n Syed Allah Bukhsh Shah r/o Tasneem Chah Basti Wazir Gadiwala, Mauza \\nTibba Mustaqil Sharqi, Kot Adu. (IW-27) \\n5.  Malik Munir Ahmad, Advocate, Kot Adu. (IW-28) \\n6.  Mr. Muhammad Ashraf Khan Rind s/o Manzoor Hussain Rind, former Nazim \\nUnion Council Bate Wala r/o Taunsa Barrage Colony, Tehsil Kot Adu. \\n(IW-29) \\n7.  \\n Mr. Ghulam Abbas s/o Muhammad Bukhsh, r/o Ward No.14-C, Mohallah \\nKhokhar Abad, Kot Adu (IW-31) \\n8.  \\n Mr. Rafique Ahmad Khan s/o Sardar Khan r/o Ward No.14-C, Kakkay Wala, \\nKot Adu. (IW-32) \\n9.  \\n Mr. Khalid Hussain Khan s/o Lal Muhammad Khan r/o Mauza Bate Qaim \\nWala, Tehsil Kot Adu. (IW-33) \\n10.  \\n Mr. Ghulam Abbas s/o Dost Muhammad r/o Chah Abbas Wala Mauza \\nHanjrai Ghair Mustaqil, Sharqi, Kot Adu.( IW-34) \\n11.  \\n Mr. Qadir Bukhsh s/o Hamid Khan, r/o Basti Nutkani, Mauza Hanjrai Ghair \\nMustaqil, Daira Deen Panah near LMB Abbas Wala, Kot Adu (IW-35) \\n12.  \\n Mr. Wahid Bukhsh s/o Ghulam Haider r/o Chah Abbas Wala, Mauza Hanjrai \\nGhair Mustaqil Sharqi, Kot Adu.  (IW-36) \\n13.  \\n Mr. Muhammad Mahiwal s/o Fateh Muhammad r/o Ward No.14-A, \\nMohallah Mandi Mawashian, Kot Adu (IW-37) \\n14.  Mr. Muhammad Bukhsh s/o Qadir Bukhsh r/o Chak Abbas Wala, Hanjrai \\nGhair Mustaqil Sharqi, Kot Addu. (IW-38) \\n \\n15.  Malik Muhammad Ibrahim Hanjra s/o Muhammad Ismail, Ex-Nazim, Union \\nCouncil Hanjra and News Reporter Nawa-e-Waqt, r/o Daira Deen Panah, \\nKot Adu. (IW-39) \\n \\n16.  \\n Mr. Sabir Hussain s/o Ghulam Sarwar r/o Mauza Tapal, Tehsil Kot Adu \\n(IW-40).  \\n17.  Mr. Ejaz Hussain s/o Ghulam Hassan r/o Chah Kandhi Wala, Mauza Kacha', metadata={'source': 'Ch2.pdf', 'page': 5}), Document(page_content='The T ribunal met f or the fir st time on 4-9-2010 and c ommenced f ormal pr oceedings  on \\n14.09.2010 under Section 8 of the Or dinance. \\n2.1. On 21.09.2010, Mr . Mansoob Ali Zaidi (Member) s tepped down fr om the T ribunal on 5\\nthe pr etext of pr essing prior c ommitmen ts. As a r eplacemen t, Mr . Shaf qat Masood, Ex-Chief \\nEngineer , I&P Departmen t and e x-Chairman and e x-Member , Indus Riv er System Authority \\n(IRSA), was appoin ted as the new Member of the T ribunal, vide notific ation da ted 25-9-6\\n2010 ,\\n3.1. The T ribunal of Inquiry (during the hearings and) f or this R eport shall be r eferred to \\nas the JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010 (“ Tribunal”). \\n3.2. The T ribunal decided t o keep the hearings priv ate (as opposed t o open hearings) in \\norder t o enc ourage and pr ovide c onfidence t o the flood af fectees and other c oncerned \\npersons t o come  forwar d and boldly depose bef ore the T ribunal, without the risk and f ear of \\nbeing e xposed or influenced b y stronge r elemen ts of the society . Priv ate hearing w as also \\nmore suit able f or carrying out cr oss e xamina tion of the witnesses.  \\n3.3.  Quorum of the T ribunal w as fix ed as tw o member s inclusiv e of the Chairman. This \\nwas only when the thir d member w as unable t o attend due t o an y disability including \\nresigna tion fr om the T ribunal.  It is f or this r eason tha t the T ribunal c ontinued as a T wo \\nMember s Tribunal when Mr . Mansoob Ali Zaidi s tepped down on 21.09.2010 till Mr . Shaf qat \\nMasood (the new member) joined the T ribunal on 03.10.2010.  On 24-11-2010 the T ribunal \\ncomplet ed its scheduled hearings and authoriz ed the Chairman t o cal l additional witnesses \\nor seek clarific ation on the evidence alr eady on the r ecord without the cons titution of the \\nTribunal, hence r edefining the quorum t o be the Chairman f or the abov e limit ed purpose.  \\nThe Member s were duly inf ormed of all the hearings held aft er 24-11-2010. \\n \\n4.1.   Inquiry unf olded tha t the “ causes” of br each w ere at sev eral involving f ailur e of \\npublic r esponsibility a t multiple lev els by dif ferent public of ficer s in dif ferent public \\ninstitutions.  It tr anspir ed tha t this ov ert failur e was mer ely s ympt oma tic of a chr onic \\ndysfunctional ins titutional s tructur e.  Inquiry of the “Causes” of br eaches c ould not be \\ncomplet e if r estricted to mer ely r egula tory and technic al reasons tha t circulated ar ound the \\nevent but r equir ed a “thinking behind floods” appr oach, a deeper pr obe t o discern if ther e 2. RE-CONS TITUTION OF THE TRIBUNAL.\\n3. PROCEDURE OF THE TRIBUNAL\\n4. SCOPE OF THE TERMS OF REFERENCE OF THE TRIBUNAL\\n5\\n Schedule- 36\\n Schedule-4was mor e than met the e ye. Without pr obing in to the ar chitectur e of flood go vernance this \\ninquiry would ha ve been half-bak ed, inchoa te and c osmetic.\\n4.2.   The T ORs w ere further qualified by the T ribunal t o setup a meaningful r oadmap f or \\nits in vestigation in to the c auses of the br eaches. They ar e; \\n4.2.1.  (AUI) t o mean: P ortion of Riv er Indus witin the Pr ovince \\nof Punjab i.e., s tarting fr om Jinnah Barr age down t o ups tream Guddu Barr age. \\n4.2.2. Breaches t o mean br each of  L GB a t Jinnah Barr age, LMB a t Taunsa Barr age, \\nJampur Bund and F akhar Flood Bund in Dis trict Rajanpur .  Breaches in Canal \\nNetw ork, R oads, Bunds and Dr ains w ere a dir ect c onsequence of the br each of LMB \\nat Taunsa Barr age and hence ar e not discussed separ ately. \\n4.2.3. To fully in vestigate whether ther e are technic al, adminis trative and \\ninstitutional c auses of br each and t o also discern if ther e is an y politic al intervention \\nor pr essur e tha t migh t have trigger ed adminis trative or ins titutional f ailur e. \\n4.2.4. To fix r esponsibility on the delinquen t(s) in case s of malf easance. Malf easance \\nshall mean wr ong doing of a public of ficial or br each of trus t by a public of ficial.  \\n4.2.5. Under  the T ribunal has f ormula ted \\nfundamen tal recommenda tions f or the futur e of flood r esilience and flood risk \\nmanag emen t in the Pr ovince. \\n5.1.  Tribunal ensur ed to provide bes t possible public access t o the flood af fectees, so tha t \\nthey c ould con venien tly reach out t o the T ribunal and mak e their submissions. \\n7\\n5.2. In or der t o hear the griev ances of the flood af fectees/public c omplainan ts, the \\nTribunal decided t o hold hearings in the af fected Dis tricts of Ar ea Under Inquiry .  For this 8\\npurpose public notices  were issued in the na tional dailies and loc al new spaper s of the \\nrespectiv e dis tricts giving a det ailed pr ogramme of the T ribunal and its visit t o the said \\ndistricts with the additional f acility of r egistering or submit ting their griev ances with the \\nlocal learned Civil Judg es. \\n5.3.  Concerned Dis trict and Sessions Judg es w ere dir ected to nomina te learned civil \\njudge fr om ev ery distri ct (namely: Mian wali, Bakk ar, Layyah, Muzz afrgarh, DG Khan and \\nRajanpur) t o receiv e complain ts on behalf of the T ribunal fr om the g ener al public. The lis t of \\nlearned civil judg es and the c omplainan ts ar e as under;“Area under Inquiry ”\\n“any other r ecommenda tions, ”\\n5. PROCEEDINGS OF THE TRIBUNAL.\\n7\\n Schedule-58\\n Schedule-6REPORT OF THE JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010   CHAPTER 2 CHAPTER 2   REPORT OF THE JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010   32 33', metadata={'source': 'Ch2.pdf', 'page': 2}), Document(page_content='The T ribunal met f or the fir st time on 4-9-2010 and c ommenced f ormal pr oceedings  on \\n14.09.2010 under Section 8 of the Or dinance. \\n2.1. On 21.09.2010, Mr . Mansoob Ali Zaidi (Member) s tepped down fr om the T ribunal on 5\\nthe pr etext of pr essing prior c ommitmen ts. As a r eplacemen t, Mr . Shaf qat Masood, Ex-Chief \\nEngineer , I&P Departmen t and e x-Chairman and e x-Member , Indus Riv er System Authority \\n(IRSA), was appoin ted as the new Member of the T ribunal, vide notific ation da ted 25-9-6\\n2010 ,\\n3.1. The T ribunal of Inquiry (during the hearings and) f or this R eport shall be r eferred to \\nas the JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010 (“ Tribunal”). \\n3.2. The T ribunal decided t o keep the hearings priv ate (as opposed t o open hearings) in \\norder t o encour age and pr ovide c onfidence t o the flood af fectees and other c oncerned \\npersons t o come  forwar d and boldly depose bef ore the T ribunal, without the risk and f ear of \\nbeing e xposed or influenced b y stronge r elemen ts of the society . Priv ate hearing w as also \\nmore suit able f or carrying out cr oss e xamina tion of the witnesses.  \\n3.3.  Quorum of the T ribunal w as fix ed as tw o member s inclusiv e of the Chairman. This \\nwas only when the thir d member w as unable t o attend due t o an y disability including \\nresigna tion fr om the T ribunal.  It is f or this r eason tha t the T ribunal c ontinued as a T wo \\nMember s Tribunal when Mr . Mansoob Ali Zaidi s tepped down on 21.09.2010 till Mr . Shaf qat \\nMasood (the new member) joined the T ribunal on 03.10.2010.  On 24-11-2010 the T ribunal \\ncomplet ed its scheduled hearings and authoriz ed the Chairman t o cal l additional witnesses \\nor seek clarific ation on the evidence alr eady on the r ecord without the cons titution of the \\nTribunal, hence r edefining the quorum t o be the Chairman f or the abov e limit ed purpose.  \\nThe Member s were duly inf ormed of all the hearings held aft er 24-11-2010. \\n \\n4.1.   Inquiry unf olded tha t the “ causes” of br each w ere at sev eral involving f ailur e of \\npublic r esponsibility a t multiple lev els by dif ferent public of ficer s in dif ferent public \\ninstitutions.  It tr anspir ed tha t this ov ert failur e was mer ely s ympt oma tic of a chr onic \\ndysfunctional ins titutional s tructur e.  Inquiry of the “Causes” of br eaches c ould not be \\ncomplet e if r estricted to mer ely r egula tory and technic al reasons tha t circulated ar ound the \\nevent but r equir ed a “thinking behind floods” appr oach, a deeper pr obe t o discern if ther e 2. RE-CONS TITUTION OF THE TRIBUNAL.\\n3. PROCEDURE OF THE TRIBUNAL\\n4. SCOPE OF THE TERMS OF REFERENCE OF THE TRIBUNAL\\n5\\n Schedule- 36\\n Schedule-4was mor e than met the e ye. Without pr obing in to the ar chitectur e of flood go vernance this \\ninquiry would ha ve been half-bak ed, inchoa te and c osmetic.\\n4.2.   The T ORs w ere further qualified by the T ribunal t o setup a meaningful r oadmap f or \\nits in vestigation in to the causes of the br eaches. They ar e; \\n4.2.1.  (AUI) t o mean: P ortion of Riv er Indus witin the Pr ovince \\nof Punjab i.e., s tarting fr om Jinnah Barr age down t o ups tream Guddu Barr age. \\n4.2.2. Breaches t o mean br each of  L GB a t Jinnah Barr age, LMB a t Taunsa Barr age, \\nJampur Bund and F akhar Flood Bund in Dis trict Rajanpur .  Breaches in Canal \\nNetw ork, R oads, Bunds and Dr ains w ere a dir ect c onsequence of the br each of LMB \\nat Taunsa Barr age and hence ar e not discussed separ ately. \\n4.2.3. To fully in vestigate whether ther e are technic al, adminis trative and \\ninstitutional c auses of br each and t o also discern if ther e is an y politic al intervention \\nor pr essur e tha t migh t have trigger ed adminis trative or ins titutional f ailur e. \\n4.2.4. To fix r esponsibility on the delinquen t(s) in case s of malf easance. Malf easance \\nshall mean wr ong doing of a public of ficial or br each of trus t by a public of ficial.  \\n4.2.5. Under  the T ribunal has f ormula ted \\nfundamen tal recommenda tions f or the futur e of flood r esilience and flood risk \\nmanag emen t in the Pr ovince. \\n5.1.  Tribunal ensur ed to provide bes t possible public access t o the flood af fectees, so tha t \\nthey c ould con venien tly reach out t o the T ribunal and mak e their submissions. \\n7\\n5.2. In or der t o hear the griev ances of the flood af fectees/public c omplainan ts, the \\nTribunal decided t o hold hearings in the af fected Dis tricts of Ar ea Under Inquiry .  For this 8\\npurpose public notices  were issued in the na tional dailies and loc al new spaper s of the \\nrespectiv e dis tricts giving a det ailed pr ogramme of the T ribunal and its visit t o the said \\ndistricts with the additional f acility of r egistering or submit ting their griev ances with the \\nlocal learned Civil Judg es. \\n5.3.  Concerned Dis trict and Sessions Judg es w ere dir ected to nomina te learned civil \\njudge fr om ev ery distri ct (namely: Mian wali, Bakk ar, Layyah, Muzz afrgarh, DG Khan and \\nRajanpur) t o receiv e complain ts on behalf of the T ribunal fr om the g ener al public. The lis t of \\nlearned civil judg es and the c omplainan ts ar e as under;“Area under Inquiry ”\\n“any other r ecommenda tions, ”\\n5. PROCEEDINGS OF THE TRIBUNAL.\\n7\\n Schedule-58\\n Schedule-6REPORT OF THE JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010   CHAPTER 2 CHAPTER 2   REPORT OF THE JUDICIAL FL OOD INQUIR Y TRIBUNAL, 2010   32 33', metadata={'source': 'Ch2.pdf', 'page': 3})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       R  C                                               R  OR  B       C    C                                             I                               \n",
            "   right     R       B                         c  [  \\        R \n",
            " \n",
            "\n",
            " B \n",
            "  \n",
            "\n",
            "  C                   OR  B B B B  B B B\n",
            " B  C  B B \n",
            " i B  B R\n",
            " R B   F B  R  B  B   I  B B  O B ? --------  B  [ \n",
            "  C \n",
            " B \\  C     B  B c  F R  i    IN  OR   C  E   R  R  C  C  C  C  C  R  I  B [ and\n",
            " T\n",
            " W\n",
            " S  B B    [   [  \n",
            "  \n",
            " i  [  M  B \n",
            " \n",
            " E  A E N \n",
            " B c[ B _%\n",
            "\n",
            " A F\n",
            "\n",
            "\n",
            " P C C R R R \n",
            " IN   R  R  C    C C  C  R S\n",
            "  R R                                        R  C                                               R  OR  B       C    C                                             I                               \n",
            "   right     R       B                         c  [  \\        R \n",
            " \n",
            "\n",
            " B \n",
            "  \n",
            "\n",
            "  C                   OR  B B B B  B B B\n",
            " B  C  B B \n",
            " i B  B R\n",
            " R B   F B  R  B  B   I  B B  O B ? --------  B  [ \n",
            "  C \n",
            " B \\  C     B  B c  F R  i    IN  OR   C  E   R  R  C  C  C  C  C  R  I  B [ and\n",
            " T\n",
            " W\n",
            " S  B B    [   [  \n",
            "  \n",
            " i  [  M  B \n",
            " \n",
            " E  A E N \n",
            " B c[ B _%\n",
            "\n",
            " A F\n",
            "\n",
            "\n",
            " P C C R R R \n",
            " IN   R  R  C    C C  C  R S\n",
            "  R R \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
        "\n",
        "qa_chain  = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n",
        "qa_chain(question)"
      ],
      "metadata": {
        "id": "s0lrcc6zsliO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.clear()"
      ],
      "metadata": {
        "id": "EKEsKs5Pf8WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "id": "jsBKPBx2hV4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain(\"what is Pakistan\")\n"
      ],
      "metadata": {
        "id": "RhSlah1efpQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.chat_memory.messages)"
      ],
      "metadata": {
        "id": "XKkeuflhb8nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install lsb-release curl gpg"
      ],
      "metadata": {
        "id": "HJ3Ge3mHPzoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://packages.redis.io/gpg"
      ],
      "metadata": {
        "id": "2Az3EXkRQAZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg"
      ],
      "metadata": {
        "id": "WZw7BcNFQCg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list"
      ],
      "metadata": {
        "id": "ejsN31M3QIFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install redis"
      ],
      "metadata": {
        "id": "KDqNbjYKQMoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install lynx"
      ],
      "metadata": {
        "id": "2VIig6DnQS8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lynx http://localhost:8000"
      ],
      "metadata": {
        "id": "q4KWKGWKTqoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chainlit"
      ],
      "metadata": {
        "id": "-fBWbJliU9DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chainlit as cl\n",
        "@cl.on_message\n",
        "async def main(query: str):\n",
        " docs = retriever.get_relevant_documents(query)\n",
        " print((llm_chain.memory).chat_memory.messages)\n",
        " output = llm_chain.run(content=docs,query=query)\n",
        " memory.save_context({\"input\": query}, {\"output\": output})\n",
        " await cl.Message(\n",
        "        content=f\"Received: {message}\",\n",
        "    ).send()"
      ],
      "metadata": {
        "id": "eVU5vHlIU3TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit run app.py -w"
      ],
      "metadata": {
        "id": "V-J1U8R5WO5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!redis-cli"
      ],
      "metadata": {
        "id": "u_b2o5TTXiSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d052vy8pmHLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}